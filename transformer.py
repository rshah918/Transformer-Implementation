# -*- coding: utf-8 -*-
"""Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RduVad6zl5szO1qE3FnqGnD_prUzrXmi

# **Transformer Scratch Implementation**
## Created by Rahul Shah

## What do attention mechanisms do?


*   Attention decides how closely each input patch is related to every other input patch. This allows you to process entire sequences at once, instead of one timestep at a time.

## Scaled Dot Product Attention:

\begin{align}
        Attention(Q,K,V) = \frac{Softmax(QK^T)} {\sqrt N}V
    \end{align}

*   Q = Query: Input vector that you want to calculate attention for
*   K = Key: Vector that you want to calculate attention against
  * For self-attention, Key = Query
  * For machine translation, Key would be an english sentence and Value would be a spanish sentence
*   V = Value: Same as the key. You multiply the Query-Key product by this matrix to get the final attention scores.


## Multi-Head Attention
Transformers use multiple Attention heads in parallel. The Key, Value, and Query matrices are multiplied by their own weight matrices.

\begin{align}
        MultiHead(Q,K,V) = [Head_1, Head_2... Head_i]*W^O
    \end{align}

\begin{align}
    Head_i = Attention(Q_iW^Q_i,K_iW^K_i,V_iW^V_i)
    \end{align}

\begin{align}
        Attention(Q,K,V) = \frac{Softmax(QK^T)} {\sqrt N}V
    \end{align}

---


# Implementation


---

#Imports
"""

import torch
import torch.nn as nn
import copy
import math
import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim

"""Helper function to duplicate torch modules. This is useful because the encoder and decoder stacks have multiple layers"""

def duplicateLayers(module, N):
    "Return a ModuleList containing N duplicate layers"
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

"""### Implementation of the FeedForward sub-layer

\begin{align}
        FeedForward(x) = max(0, XW_1+b_1)*W_2b_2
    \end{align}
*   Implemented as 2 Dense layers, with Relu activation for the first dense layer.
*   Length of W_1 is defined by d_ff
*   Length of W_2 is defined by d_model
"""

class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = self.w_1(x)
        out = nn.functional.relu(out)
        out = self.dropout(out)
        out = self.w_2(out)
        return out

"""Layer Normalization implementation"""

class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

"""### Implementation of Multi-Headed Self-Attention"""

def attention(Q,K,V,mask=None):
    dimension_k = Q.shape[-1]
    attention_scores = torch.matmul(Q, K.transpose(-2, -1))
    attention_scores = attention_scores / math.sqrt(dimension_k)
    #apply a mask
    if mask is not None:
      attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
    #do a row-wise softmax
    attention_scores = nn.functional.softmax(attention_scores, dim = -1)
    #multiply attention scores by V and return
    return torch.matmul(attention_scores, V)

class MultiHeadedAttention(nn.Module):
    def __init__(self, num_heads, d_model):
          super(MultiHeadedAttention, self).__init__()
          #number of attention heads must be divisible by the input dimension
          assert(d_model % num_heads == 0)
          self.num_heads = num_heads
          self.d_k = d_model // num_heads
          self.linears = duplicateLayers(nn.Linear(d_model, d_model), 4)
          #contains the outputs of the h attention heads
          self.attn = None

    def forward(self, query, key, value, mask=None):
        "Implements Figure 2"
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = \
            [l(x).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)
             for l, x in zip(self.linears, (query, key, value))]

        # 2) Apply attention on all the projected vectors in batch.
        x = attention(query, key, value, mask=mask)

        # 3) "Concat" using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.num_heads * self.d_k)
        return self.linears[-1](x)

"""### Implement Encoder Stack

"""

class EncoderLayer(nn.Module):
  def __init__(self, feedforward, attention, layernorm):
    super(EncoderLayer, self).__init__()
    self.feedforward = feedforward
    self.attention = attention
    self.norm = layernorm

  def forward(self, x, mask=None):
    self_attention = self.attention.forward(x,x,x, mask)
    #skip connection and then layer normalization
    self_attention = self.norm.forward(self_attention + x)
    #feed forward
    out = self.feedforward.forward(self_attention)
    #skip connection and layer normalization
    out = self.norm.forward(self_attention + out)
    return out

class Encoder(nn.Module):
    def __init__(self, d_model, d_ff, num_attention_heads, num_encoder_layers):
        super(Encoder, self).__init__()
        #instantiate an encoder layer
        self.encoder_layer = EncoderLayer(PositionwiseFeedForward(d_model, d_ff), MultiHeadedAttention(num_attention_heads,d_model), LayerNorm(d_model))
        #according to the paper, the encoder stack has 6 encoder layers
        self.layers = duplicateLayers(self.encoder_layer, num_encoder_layers)
    def forward(self, x, mask=None):
        #sequentially pass the input through each encoder layer
        for layer in self.layers:
            x = layer(x, mask)
        return x

"""### Implement Decoder Stack

"""

class DecoderLayer(nn.Module):
    def __init__(self, feedforward, attention, layernorm):
        super(DecoderLayer, self).__init__()
        self.feedforward = feedforward
        self.attention = attention
        self.norm = layernorm

    def forward(self, output, encoder_output, mask=None):
        #pass output into self attention module
        query = self.attention.forward(output, output, output, mask)
        #skip connection and layer normalize
        query = self.norm.forward(query + output)
        #pass value and encoder output into another attention module
        #Key,Value is the encoder output, Query is the previous attention module's output
        self_attention = self.attention.forward(query, encoder_output, encoder_output, mask)
        #skip connection and layer normalize
        self_attention = self.norm.forward(self_attention + query)
        #feed forward
        out = self.feedforward.forward(self_attention)
        #skip connection and layer normalize
        out = self.norm.forward(out + self_attention)
        return out

class Decoder(nn.Module):
    def __init__(self, d_model, dff, num_attention_heads, num_decoder_layers, vocab_size):
        super(Decoder, self).__init__()
        #instantiate a decoder layer
        self.decoder_layer = DecoderLayer(PositionwiseFeedForward(d_model, dff), MultiHeadedAttention(num_attention_heads, d_model), LayerNorm(d_model))
        #according to the paper, the decoder stack has 6 decoder layers
        self.layers = duplicateLayers(self.decoder_layer, num_decoder_layers)
        self.linear = nn.Linear(d_model, vocab_size)

    def forward(self, x, encoder_output, mask=None):
        #pass decoder input through each decoder layer sequentially
        for layer in self.layers:
            x = layer(x, encoder_output, mask)
        #pass through a dense layer
        out = self.linear(x)
        #pass through tanh
        out = torch.tanh(out)
        return out

"""### Connect the Encoder and Decoder stacks into a single Transformer"""

class Transformer(nn.Module):
  def __init__(self, d_model, d_ff,num_attention_heads, num_decoder_layers, vocab_size):
    super(Transformer, self).__init__()
    self.encoder = Encoder(d_model, d_ff,num_attention_heads, num_encoder_layers)
    self.decoder = Decoder(d_model, d_ff,num_attention_heads, num_decoder_layers, vocab_size)

  def forward(self,x,mask=None):
    out = self.encoder.forward(x, mask)
    out = self.decoder.forward(x, out, mask)
    return out

"""# Generate Dataset
#### I will train this implementation to learn a sine wave

Create Sine Wave
"""

sin_wave = np.array([math.sin(x) for x in np.arange(200)])
plt.plot(sin_wave[:50])

"""Create Dataset"""

X = []
Y = []

seq_len = 50
num_records = len(sin_wave) - seq_len

for i in range(num_records - seq_len):
    X.append(sin_wave[i:i+seq_len])
    Y.append(sin_wave[i+seq_len])

X = np.array(X)
X = np.expand_dims(X, axis=2)
X = torch.Tensor(X)

Y = np.array(Y)
Y = np.expand_dims(Y, axis=1)
Y = torch.Tensor(Y)

"""Verify Dataset shape"""

X.shape, Y.shape

"""#Train

Create Training Loop
"""

NUM_EPOCHS = 4
TIMESTEPS = 50

d_model = 50
d_ff = 64
vocab_size=1
num_attention_heads = 1
num_encoder_layers = 1
num_decoder_layers = 1
transformer = Transformer(d_model, d_ff,num_attention_heads, num_decoder_layers, vocab_size)
optimizer = optim.SGD(transformer.parameters(), lr=0.001, momentum=0.9)
loss = nn.MSELoss()
for epoch in range(NUM_EPOCHS):
  running_loss = 0.0
  for i in range(Y.shape[0]):
    x, y = X[i], Y[i]
    out = transformer.forward(x.T)
    running_loss += (y - float(out[0][0][0]))**2 / 2
  error = loss(out[0][0], y)
  error.backward()
  optimizer.step()
  running_loss = running_loss / float(Y.shape[0])
  print("Loss: ", running_loss)

"""#Test Transformer"""

#test the neural net on validation set
x = []
y = []
for i in range(40):
  x.append(i)
  output = transformer.forward(X[i].T)
  y.append(float(output))
#plot nn prediction and validation set
plt.plot(x,y, label="prediction")
plt.plot(Y[:40], label="ground truth")
plt.title("Transformer prediction vs Ground Truth")
leg = plt.legend(loc='upper right')
plt.show()
